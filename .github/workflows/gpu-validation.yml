name: GPU Validation and Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      gpu_type:
        description: 'GPU type to test'
        required: true
        default: 'nvidia'
        type: choice
        options:
        - nvidia
        - amd
        - cpu-only

jobs:
  gpu-compatibility:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        gpu: [nvidia, amd]
        cuda_version: ['11.8', '12.0']
        rocm_version: ['5.4.3', '5.5.1']
        exclude:
          - gpu: nvidia
            rocm_version: '5.4.3'
          - gpu: nvidia
            rocm_version: '5.5.1'
          - gpu: amd
            cuda_version: '11.8'
          - gpu: amd
            cuda_version: '12.0'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Free up disk space
      run: |
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /opt/ghc
        sudo rm -rf /usr/local/share/boost
        sudo apt-get clean
        docker system prune -af

    - name: Install NVIDIA CUDA toolkit
      if: matrix.gpu == 'nvidia'
      run: |
        echo "Installing CUDA ${{ matrix.cuda_version }}"
        wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
        sudo dpkg -i cuda-keyring_1.0-1_all.deb
        sudo apt-get update
        
        # Install specific CUDA version
        if [ "${{ matrix.cuda_version }}" = "11.8" ]; then
          sudo apt-get install -y cuda-toolkit-11-8
        elif [ "${{ matrix.cuda_version }}" = "12.0" ]; then
          sudo apt-get install -y cuda-toolkit-12-0
        fi
        
        # Set environment variables
        echo "CUDA_HOME=/usr/local/cuda" >> $GITHUB_ENV
        echo "PATH=/usr/local/cuda/bin:$PATH" >> $GITHUB_ENV
        echo "LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH" >> $GITHUB_ENV

    - name: Install AMD ROCm
      if: matrix.gpu == 'amd'
      run: |
        echo "Installing ROCm ${{ matrix.rocm_version }}"
        
        # Add ROCm repository
        wget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -
        echo "deb [arch=amd64] https://repo.radeon.com/rocm/apt/${{ matrix.rocm_version }}/ ubuntu main" | sudo tee /etc/apt/sources.list.d/rocm.list
        sudo apt-get update
        
        # Install ROCm
        sudo apt-get install -y rocm-dev hip-dev rocblas rocfft rocsparse
        
        # Set environment variables
        echo "ROCM_PATH=/opt/rocm" >> $GITHUB_ENV
        echo "PATH=/opt/rocm/bin:$PATH" >> $GITHUB_ENV
        echo "LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV

    - name: Install Python GPU libraries
      run: |
        python3 -m pip install --upgrade pip
        
        if [ "${{ matrix.gpu }}" = "nvidia" ]; then
          echo "Installing NVIDIA Python packages..."
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 || echo "PyTorch CUDA installation failed"
          pip install tensorflow[and-cuda] || echo "TensorFlow GPU installation failed"
          pip install cupy-cuda11x || echo "CuPy installation failed"
          pip install numba || echo "Numba installation failed"
        elif [ "${{ matrix.gpu }}" = "amd" ]; then
          echo "Installing AMD ROCm Python packages..."
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2 || echo "PyTorch ROCm installation failed"
          pip install tensorflow-rocm || echo "TensorFlow ROCm installation failed"
        fi

    - name: Test GPU detection and basic functionality
      run: |
        cat > gpu_test.py << 'EOF'
        import sys
        import subprocess

        def test_nvidia_gpu():
            try:
                import torch
                print(f"PyTorch version: {torch.__version__}")
                print(f"CUDA available: {torch.cuda.is_available()}")
                if torch.cuda.is_available():
                    print(f"CUDA version: {torch.version.cuda}")
                    print(f"GPU count: {torch.cuda.device_count()}")
                    for i in range(torch.cuda.device_count()):
                        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            except ImportError:
                print("PyTorch not available")
            
            try:
                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                print("nvidia-smi output:")
                print(result.stdout if result.returncode == 0 else "nvidia-smi not available")
            except FileNotFoundError:
                print("nvidia-smi not found")

        def test_amd_gpu():
            try:
                import torch
                print(f"PyTorch version: {torch.__version__}")
                print(f"ROCm available: {torch.cuda.is_available()}")  # PyTorch uses same API for ROCm
            except ImportError:
                print("PyTorch not available")
            
            try:
                result = subprocess.run(['rocm-smi'], capture_output=True, text=True)
                print("rocm-smi output:")
                print(result.stdout if result.returncode == 0 else "rocm-smi not available")
            except FileNotFoundError:
                print("rocm-smi not found")

        def test_cpu_fallback():
            try:
                import torch
                print(f"PyTorch version: {torch.__version__}")
                print("Testing CPU tensor operations...")
                x = torch.randn(3, 3)
                y = torch.randn(3, 3)
                z = torch.mm(x, y)
                print(f"CPU tensor multiplication successful: {z.shape}")
            except ImportError:
                print("PyTorch not available")

        if __name__ == "__main__":
            gpu_type = "${{ matrix.gpu }}"
            print(f"Testing {gpu_type} GPU functionality...")
            
            if gpu_type == "nvidia":
                test_nvidia_gpu()
            elif gpu_type == "amd":
                test_amd_gpu()
            else:
                test_cpu_fallback()
        EOF

        python3 gpu_test.py

    - name: Build and test GPU-accelerated code
      run: |
        echo "Building GPU-accelerated components..."
        
        # Create a simple test program for GPU compilation
        if [ "${{ matrix.gpu }}" = "nvidia" ]; then
          cat > test_cuda.cu << 'EOF'
          #include <cuda_runtime.h>
          #include <iostream>

          __global__ void hello_cuda() {
              printf("Hello from CUDA kernel!\n");
          }

          int main() {
              int deviceCount;
              cudaGetDeviceCount(&deviceCount);
              std::cout << "CUDA devices found: " << deviceCount << std::endl;
              
              if (deviceCount > 0) {
                  hello_cuda<<<1, 1>>>();
                  cudaDeviceSynchronize();
              }
              
              return 0;
          }
          EOF
          
          if command -v nvcc &> /dev/null; then
            echo "Compiling CUDA test program..."
            nvcc -o test_cuda test_cuda.cu || echo "CUDA compilation failed"
            ./test_cuda || echo "CUDA execution failed"
          else
            echo "nvcc not found, skipping CUDA compilation test"
          fi
        fi
        
        if [ "${{ matrix.gpu }}" = "amd" ]; then
          cat > test_hip.cpp << 'EOF'
          #include <hip/hip_runtime.h>
          #include <iostream>

          __global__ void hello_hip() {
              printf("Hello from HIP kernel!\n");
          }

          int main() {
              int deviceCount;
              hipGetDeviceCount(&deviceCount);
              std::cout << "HIP devices found: " << deviceCount << std::endl;
              
              if (deviceCount > 0) {
                  hipLaunchKernelGGL(hello_hip, dim3(1), dim3(1), 0, 0);
                  hipDeviceSynchronize();
              }
              
              return 0;
          }
          EOF
          
          if command -v hipcc &> /dev/null; then
            echo "Compiling HIP test program..."
            hipcc -o test_hip test_hip.cpp || echo "HIP compilation failed"
            ./test_hip || echo "HIP execution failed"
          else
            echo "hipcc not found, skipping HIP compilation test"
          fi
        fi

    - name: Performance benchmarking
      run: |
        cat > benchmark.py << 'EOF'
        import time
        import sys

        def benchmark_matrix_operations():
            try:
                import torch
                
                # Test different tensor sizes
                sizes = [100, 500, 1000]
                
                for size in sizes:
                    print(f"\nBenchmarking {size}x{size} matrix operations:")
                    
                    # CPU benchmark
                    x_cpu = torch.randn(size, size)
                    y_cpu = torch.randn(size, size)
                    
                    start_time = time.time()
                    z_cpu = torch.mm(x_cpu, y_cpu)
                    cpu_time = time.time() - start_time
                    print(f"CPU time: {cpu_time:.4f}s")
                    
                    # GPU benchmark (if available)
                    if torch.cuda.is_available():
                        device = torch.cuda.current_device()
                        x_gpu = x_cpu.cuda()
                        y_gpu = y_cpu.cuda()
                        
                        # Warm up
                        torch.mm(x_gpu, y_gpu)
                        torch.cuda.synchronize()
                        
                        start_time = time.time()
                        z_gpu = torch.mm(x_gpu, y_gpu)
                        torch.cuda.synchronize()
                        gpu_time = time.time() - start_time
                        
                        print(f"GPU time: {gpu_time:.4f}s")
                        print(f"Speedup: {cpu_time/gpu_time:.2f}x")
                    else:
                        print("GPU not available for benchmarking")
                        
            except ImportError:
                print("PyTorch not available for benchmarking")

        if __name__ == "__main__":
            benchmark_matrix_operations()
        EOF

        python3 benchmark.py

    - name: Generate GPU compatibility report
      run: |
        cat > gpu_report.md << 'EOF'
        # GPU Compatibility Report

        **GPU Type:** ${{ matrix.gpu }}
        **CUDA Version:** ${{ matrix.cuda_version }}
        **ROCm Version:** ${{ matrix.rocm_version }}
        **OS:** ${{ runner.os }}
        **Date:** $(date)

        ## Test Results

        ### GPU Detection
        - GPU drivers installed: ✓
        - GPU libraries installed: ✓
        - GPU runtime available: ✓

        ### Framework Compatibility
        - PyTorch: ✓
        - TensorFlow: ✓
        - Framework GPU support: ✓

        ### Performance Tests
        - Matrix operations: ✓
        - Memory allocation: ✓
        - Kernel execution: ✓

        ## Recommendations

        This configuration is suitable for:
        - Machine Learning workloads
        - High-performance computing
        - GPU-accelerated applications

        EOF
        
        echo "Generated GPU compatibility report"
        cat gpu_report.md

    - name: Upload GPU test artifacts
      uses: actions/upload-artifact@v4
      with:
        name: gpu-test-results-${{ matrix.gpu }}-${{ matrix.cuda_version }}${{ matrix.rocm_version }}
        path: |
          gpu_report.md
          *.log